#Systematic Review with LSTM. Copy of de Imdb sentiment example
#author: Kees van Eijden
#version/date:  0.1/05-feb-2018

#input files (.csv) are assumed to be in the working directory

#reading "Initial Search" data (is)
library(jsonlite)
cat("Stage 1: Cleaning and exploring Initial Search data.")
is_raw_csv<- read.csv(file = "./01. Initial Search.csv",
                header= TRUE, sep= ",", stringsAsFactors = FALSE)
library(dplyr)
is <- select(is_raw_csv, abstract, title, en_id=  id) #for now only abstract, title and en_id
                                          #(EndNote ID) are of interest

#exploring the data in the initial search
cat("Exploring the data in Initial Search")
n <- summarize(is, num= n())
cat("...#articles in initial search: ", n$num, "")

no_title <- filter(is, title == "") %>% summarize(n())
no_abstract <- filter(is, abstract == "") %>% summarize(n())

cat("...#articles without abstract: ", no_abstract[[1]])

if (no_title[[1]] > 0) {
    cat("...#articles without title: ", no_title[[1]], "..written to file.")
    articles_without_title <- is %>% filter(title == "")
    write.csv( articles_without_title, "./withoutTitles.csv")
}
if (no_abstract[[1]] > 0) {
    cat("...#articles without abstract: ", no_abstract[[1]], "..written to file")
    articles_without_abstracts <- is %>% filter(abstract == "")
    write.csv(articles_without_abstracts, "./withoutAbstracts")
}

cat("...deleting all articles without title or abstract")
is <- filter(is, ((title != "") & (abstract != "")))

n <- summarise(is, num = n(), num_d= n_distinct(abstract, title),
               num_a= n_distinct(abstract), num_t= n_distinct(title))

cat("Articles left: ", n$num)
cat("Distinct articles: ", n$num_d)
cat("Distinct abstracts: ", n$num_a)
cat("Distinct titles: ", n$num_t)

dup_articles <- is %>% group_by(title, abstract) %>% summarize(n=n(), en_id= min(en_id)) %>% filter(n >1)
if ((dup= nrow(dup_articles)) > 0) {
    cat("...#duplicate articles (title&abstract): ", dup, "..written to file")
    dup_articles1 <- semi_join(is, dup_articles, by=c("abstract", "title"))
    write.csv(dup_articles1, "./duplicateArticles.csv")
    #keep only one occurence of article; the one with lowest endnote id
    is <- left_join(is, dup_articles, by= c("abstract", "title"))
    is <- is %>% filter((en_id.x == en_id.y) | (is.na(en_id.y))) %>% select(en_id=en_id.x, title, abstract)
}



#titles with multiple occurences
dup_titles <- is %>% group_by(title) %>% summarize(n= n(), en_id= min(en_id)) %>% filter(n > 1)
if ((dup= nrow(dup_titles)) > 0) {
    cat("...#duplicate titles: ", dup, "..written to file")
    dup_titles1 <- semi_join(is, dup_titles, by=c("title"))
    write.csv(dup_titles1, "./duplicateTitles.csv")
    #keep only one occurence of title; the one with lowest endnote ID
    is <- left_join(is, dup_titles, by= c("title"))
    is <- is %>% filter((en_id.x == en_id.y) | (is.na(en_id.y))) %>% select(en_id=en_id.x, title, abstract)
}


#abstracts with multiple occurences
dup_abstracts <- is %>% group_by(abstract) %>% summarize(n= n(), en_id= min(en_id)) %>% filter(n > 1)
if ((dup= nrow(dup_abstracts)) > 0) {
    cat("...#duplicate abstracts: ", dup, "..written to file")
    dup_abstracts1 <- semi_join(is, dup_abstracts, by=c("abstract"))
    write.csv(dup_abstracts, "./duplicateAbstracts.csv")
    #keep only one occurence of abstract; the one with lowest endnote id
    is <- left_join(is, dup_abstracts, by= c("abstract"))
    is <- is %>% filter((en_id.x == en_id.y) | (is.na(en_id.y))) %>% select(en_id= en_id.x, title, abstract)
}

#write out the cleansed Initial Search data
num_is <- nrow(is)
cat("...write out ", num_is , "articles to file cleanInitiaSearch.csv")
write.csv(is, "./cleanInitialSearch.csv")

#Stage 2: Import an clean Selected Articles after Title screening
#In this stage the user will select articles based on the title screening. This will be an interactive
#procedure. For the time being we import a file with the selected articles after title screening.
#If a article title in
#the initial search is present in the list of selected articles, the label sentiment_ats
#(sentiment after title screening) gets the value 1; otherwise 0
ats_raw <- read.csv(file = "./02. included after title screening.csv",
               header= TRUE, sep= ",", stringsAsFactors = FALSE)
ats <- select(ats_raw, title, en_id = id) #we only use titles and id; en_id means the id generated by EndNote

cat("Stage 2: Processing selected articles after title screening.")
cat("...#articles in input file: ", nrow(ats))
no_title <- ats %>% filter(title == "") %>% summarise(n=n())
if (no_title[[1]] != 0) {
    cat("...#empty titles in ATS file: ", no_title[[1]] , "..endnote id's written to file")
    no_titles <- ats_raw %>% filter(title == "")
    write.csv(no_titles, "./noTitlesATS.csv")
    cat("...deleting articles with empty titles.")
    ats <- ats %>% filter(title != "")
}

dup_titles <- ats %>% group_by(title) %>% summarise(n=n(), en_id=min(en_id)) %>% filter(n > 1)
if ((dup=nrow(dup_titles)) > 0) {
    cat("...#duplicate titles in ATS: ", dup, ".. writen to file")
    dup_titles1 <- semi_join(ats, dup_titles, by=c("title"))
    write.csv(dup_titles1, "./duplicateTitlesATS.csv")
    #keep only one occurence of title; the one with lowest endnote ID
    ats <- left_join(ats, dup_titles, by= c("title"))
    ats <- ats %>% filter((en_id.x == en_id.y) | (is.na(en_id.y))) %>% select(en_id=en_id.x, title)
}

##Which selected articles do not have an corresponding title in the initial search
not_in_initial <- anti_join(ats, is, by=c("title"))
if ((nii=nrow(not_in_initial)) > 0) {
    cat("...#articles (ATS) not in initial serach: ", nii, "..written to file and deleted")
    write.csv(not_in_initial, "./notInInitSearchATS.csv")
    ats <- anti_join(ats, not_in_initial, by=c("title"))
}

cat("...#selected articles ATS after cleaning: ", nrow(ats), "..written to file")
write.csv(ats, "./cleanedATS.csv")

##Stage 3: Join initial search articles and articles after title screening
##if article in both sets it gets snentiment = 1 else sentiment = 0

ats$sentiment_ats <- 1       #sentiment after title screening

train_data <-left_join(is, ats, by=c("title"))

train_data$sentiment_ats[is.na(train_data$sentiment_ats)]<- 0 

t <- table(train_data$sentiment_ats)
cat("...sentiment analyses of abstracts: ", t[1], " negatives and ", t[2], " features.")


##we don't have a vocabulary yet
##we use the keras tokenizer to create a vocabulary
##and corresponding (integer) tokens
##the abstracts are transformed into sequences (vectors of tokens)
library(keras)
num_words <- 5000 ### number of most important words. 5000 is just a guess!
tokenizer <- text_tokenizer(num_words = num_words)
tokenizer %>% fit_text_tokenizer(train_data$abstract)
sequences <- texts_to_sequences(tokenizer, train_data$abstract)

##tokenizer$word_index givess mapping of words to their indices(=rank)
##there are many stop words and should be filtered out (to-do)

#make train en test sets
test_size <- 100 ###just a guess
TESTSET <- sample(1:length(sequences), test_size)

x_train <- sequences[-TESTSET]
y_train <- train_data$sentiment_ats[-TESTSET]
x_test <- sequences[ TESTSET]
y_test <- train_data$sentiment_ats[ TESTSET]



y_train <- as.numeric(y_train)
y_test  <- as.numeric(y_test)

#in the example the sequences are padded and truncated. 
#we don't know yet what's the correct length for our case
max_len <- 80
x_train <- pad_sequences(x_train, maxlen = max_len)
x_test <- pad_sequences(x_test, maxlen = max_len)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')


#building the LSTM model. Just a copy of the IMDB example
model <- keras_model_sequential()
model %>%
    layer_embedding(input_dim = num_words, output_dim = 128) %>% 
    layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% 
    layer_dense(units = 1, activation = 'sigmoid')

# Try using different optimizers and different optimizer configs
model %>% compile(
    loss = 'binary_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy')
)

batch_size <- 32 #is this the coirrect size for our case?

model %>% fit(
    x_train, y_train,
    batch_size = batch_size,
    epochs = 15,
    validation_split = 0.2
    #validation_data = list(x_test, y_test)
)

scores <- model %>% evaluate(
    x_test, y_test,
    batch_size = batch_size
)

cat('Test loss on test set:', scores[[1]])
cat('Test accuracy on test set', scores[[2]])


